{
  "type": "object",
  "properties": {
    "service": {
      "type": "object",
      "description": "DC/OS service configuration properties",
      "properties": {
        "name" : {
          "description": "The name of the service instance",
          "type": "string",
          "default": "hdfs"
        },
        "virtual_network_enabled": {
          "description": "Enable virtual networking",
          "type": "boolean",
          "default": false
        },
        "virtual_network_name": {
          "description": "The name of the virtual network to join",
          "type": "string",
          "default": "dcos"
        },
        "virtual_network_plugin_labels": {
          "description": "Labels to pass to the virtual network plugin. Comma-separated key:value pairs. For example: k_0:v_0,k_1:v_1,...,k_n:v_n",
          "type": "string",
          "default": ""
        },
        "user": {
          "type": "string",
          "description": "The linux user used to run the scheduler and all executors.",
          "default": "nobody"
        },
        "service_account" : {
          "description": "The service account for DC/OS service authentication. This is typically left empty to use the default unless service authentication is needed. The value given here is passed as the principal of Mesos framework.",
          "type": "string",
          "default": ""
        },
        "mesos_api_version": {
          "description": "Configures the Mesos API version to use. Possible values: V0 (non-HTTP), V1 (HTTP)",
          "type": "string",
          "default": "V0"
        },
        "service_account_secret": {
          "description": "Name of the Secret Store credentials to use for DC/OS service authentication. This should be left empty unless service authentication is needed.",
          "type": "string",
          "default": ""
        },
        "deploy_strategy": {
          "description": "HDFS deployment strategy. [parallel, serial]",
          "type": "string",
          "default": "parallel",
          "enum" : ["parallel", "serial"]
        },
        "update_strategy": {
          "description": "HDFS update strategy. [parallel, serial]",
          "type": "string",
          "default": "serial",
          "enum" : ["parallel", "serial"]
        },
        "log_level": {
          "description": "The log level for the DC/OS service.",
          "type": "string",
          "enum": ["OFF", "FATAL", "ERROR", "WARN", "INFO", "DEBUG", "TRACE", "ALL"],
          "default": "INFO"
        },
        "tls": {
          "type": "object",
          "description": "TLS configuration properties.",
          "properties": {
            "enabled": {
              "description": "Enable TLS support (requires Strict mode).",
              "type": "boolean",
              "default": false
            }
          }
        },
        "kerberos": {
          "type": "object",
          "description": "Kerberos configuration properties.",
          "properties": {
            "enabled": {
              "description": "Enable kerberos authentication.",
              "type": "boolean",
              "default": false
            },
            "keytabs_uri": {
              "type": "string",
              "description": "The URI from which Keytabs tar.gz can be downloaded. Only relevant when secure_mode is enabled.",
              "default": ""
            },
            "krb5_conf_uri": {
              "type": "string",
              "description": "The URI of the krb5.conf file.  This file will be downloaded and used by each HDFS task."
            },
            "primary": {
              "type": "string",
              "description": "The Kerberos primary used by HDFS tasks.  The full principal will be <service.kerberos.primary>/<mesos_dns_address>@<service.kerberos.realm>",
              "default": "hdfs"
            },
            "primary_http": {
              "type": "string",
              "description": "The Kerberos primary used by the HTTP server running on HDFS tasks.  See <service.kerberos.primary>.",
              "default": "HTTP"
            },
            "realm": {
              "type": "string",
              "description": "The Kerberos realm used to render the principal of HDFS tasks.",
              "default": "LOCAL"
            }
          }
        }
      }
    },
    "journal_node": {
      "description":"HDFS configuration properties.",
      "type": "object",
      "properties": {
        "cpus": {
          "description": "Journal node CPU requirement",
          "type": "number",
          "default": 0.3
        },
        "mem": {
          "description": "Journal node memory requirement",
          "type": "number",
          "default": 2048
        },
        "disk": {
          "description": "Journal node disk size requirement in MB",
          "type": "number",
          "default": 5000
        },
        "disk_type": {
          "description": "Journal node disk type",
          "type": "string",
          "default": "ROOT"
        },
        "enable_readiness_check": {
          "description": "Determines whether to enable readiness checks for Journal Nodes.",
          "type": "boolean",
          "default": true
        },
        "lagging_tx_count": {
          "description": "The number of transactions that this JournalNode is lagging",
          "type": "integer",
          "default": 0
        }
      },
      "required": [
        "cpus",
        "mem",
        "disk",
        "disk_type"
      ]
    },
    "name_node": {
      "description": "HDFS configuration properties.",
      "type": "object",
      "properties": {
        "cpus": {
          "description": "Name node CPU requirement",
          "type": "number",
          "default": 0.3
        },
        "mem": {
          "description": "Name node memory requirement",
          "type": "number",
          "default": 2048
        },
        "disk": {
          "description": "Name node disk size requirement in MB",
          "type": "number",
          "default": 5000
        },
        "disk_type": {
          "description": "Name node disk type",
          "type": "string",
          "default": "ROOT"
        },
        "replication_considerLoad": {
          "description": "Decide if chooseTarget considers the target's load or not",
          "type": "boolean",
          "default": true
        },
        "name_dir_restore": {
          "description": "Set to true to enable NameNode to attempt recovering a previously failed dfs.namenode.name.dir. When enabled, a recovery of any failed directory is attempted during checkpoint.",
          "type": "boolean",
          "default": false
        },
        "fs-limits_max-component-length": {
          "description": "Defines the maximum number of bytes in UTF-8 encoding in each component of a path. A value of 0 will disable the check.",
          "type": "integer",
          "default": 255
        },
        "fs-limits_max-directory-items": {
          "description": "Defines the maximum number of items that a directory may contain. A value of 0 will disable the check.",
          "type": "integer",
          "default": 1048576
        },
        "fs-limits_min-block-size": {
          "description": "Minimum block size in bytes, enforced by the Namenode at create time. This prevents the accidental creation of files with tiny block sizes (and thus many blocks), which can degrade performance.",
          "type": "integer",
          "default": 1048576
        },
        "fs-limits_max-blocks-per-file": {
          "description": "Maximum number of blocks per file, enforced by the Namenode on write. This prevents the creation of extremely large files which can degrade performance.",
          "type": "integer",
          "default": 1048576
        },
        "acls_enabled": {
          "type": "boolean",
          "description": "Set to true to enable support for HDFS ACLs (Access Control Lists). By default, ACLs are disabled. When ACLs are disabled, the NameNode rejects all RPCs related to setting or getting ACLs.",
          "default": false
        },
        "lazypersist_file_scrub_interval_sec": {
          "type": "integer",
          "description": "The NameNode periodically scans the namespace for LazyPersist files with missing blocks and unlinks them from the namespace. This configuration key controls the interval between successive scans. Set it to a negative value to disable this behavior.",
          "default": 300
        },
        "replication_min": {
          "type": "integer",
          "description": "Minimal block replication.",
          "default": 1
        },
        "handler_count": {
          "type": "integer",
          "description": "The number of server threads for the namenode.",
          "default": 10
        },
        "safemode_threshold-pct": {
          "type": "string",
          "description": "Specifies the percentage of blocks that should satisfy the minimal replication requirement defined by dfs.namenode.replication.min. Values less than or equal to 0 mean not to wait for any particular percentage of blocks before exiting safemode. Values greater than 1 will make safe mode permanent.",
          "default": "0.999f"
        },
        "safemode_min_datanodes": {
          "type": "integer",
          "description": "Specifies the number of datanodes that must be considered alive before the name node exits safemode. Values less than or equal to 0 mean not to take the number of live datanodes into account when deciding whether to remain in safe mode during startup. Values greater than the number of datanodes in the cluster will make safe mode permanent.",
          "default": 0
        },
        "safemode_extension": {
          "type": "integer",
          "description": "Determines extension of safe mode in milliseconds after the threshold level is reached.",
          "default": 30000
        },
        "resource_check_interval": {
          "type": "integer",
          "description": "The interval in milliseconds at which the NameNode resource checker runs. The checker calculates the number of the NameNode storage volumes whose available spaces are more than dfs.namenode.resource.du.reserved, and enters safemode if the number becomes lower than the minimum value specified by dfs.namenode.resource.checked.volumes.minimum.",
          "default": 5000
        },
        "resource_du_reserved": {
          "type": "integer",
          "description": "The amount of space to reserve/require for a NameNode storage directory in bytes. The default is 100MB.",
          "default": 104857600
        },
        "resource_checked_volumes": {
          "type": "string",
          "description": "A list of local directories for the NameNode resource checker to check in addition to the local edits directories.",
          "default": ""
        },
        "resource_checked_volumes_minimum": {
          "type": "integer",
          "description": "The minimum number of redundant NameNode storage volumes required.",
          "default": 1
        },
        "max_objects": {
          "type": "integer",
          "description": "The maximum number of files, directories and blocks dfs supports. A value of zero indicates no limit to the number of objects that dfs supports.",
          "default": 0
        },
        "decommission_interval": {
          "type": "integer",
          "description": "Namenode periodicity in seconds to check if decommission is complete.",
          "default": 30
        },
        "decommission_nodes_per_interval": {
          "type": "integer",
          "description": "The number of nodes namenode checks if decommission is complete in each dfs.namenode.decommission.interval.",
          "default": 5
        },
        "replication_interval": {
          "type": "integer",
          "description": "The periodicity in seconds with which the namenode computes repliaction work for datanodes.",
          "default": 3
        },
        "accesstime_precision": {
          "type": "integer",
          "description": "The access time for HDFS file is precise upto this value. The default value is 1 hour. Setting a value of 0 disables access times for HDFS.",
          "default": 3600000
        },
        "plugins": {
          "type": "string",
          "description": "Comma-separated list of namenode plug-ins to be activated.",
          "default": ""
        },
        "checkpoint_period": {
          "type": "integer",
          "description": "The number of seconds between two periodic checkpoints.",
          "default": 3600
        },
        "checkpoint_txns": {
          "type": "integer",
          "description": "The Secondary NameNode or CheckpointNode will create a checkpoint of the namespace every 'dfs.namenode.checkpoint.txns' transactions, regardless of whether 'dfs.namenode.checkpoint.period' has expired.",
          "default": 1000000
        },
        "checkpoint_check_period": {
          "type": "integer",
          "description": "The SecondaryNameNode and CheckpointNode will poll the NameNode every 'dfs.namenode.checkpoint.check.period' seconds to query the number of uncheckpointed transactions.",
          "default": 60
        },
        "checkpoint_max-retries": {
          "type": "integer",
          "description": "The SecondaryNameNode retries failed checkpointing. If the failure occurs while loading fsimage or replaying edits, the number of retries is limited by this variable.",
          "default": 3
        },
        "num_checkpoints_retained": {
          "type": "integer",
          "description": "The number of image checkpoint files that will be retained by the NameNode and Secondary NameNode in their storage directories. All edit logs necessary to recover an up-to-date namespace from the oldest retained checkpoint will also be retained.",
          "default": 2
        },
        "num_extra_edits_retained": {
          "type": "integer",
          "description": "The number of extra transactions which should be retained beyond what is minimally necessary for a NN restart. This can be useful for audit purposes or for an HA setup where a remote Standby Node may have been offline for some time and need to have a longer backlog of retained edits in order to start again. Typically each edit is on the order of a few hundred bytes, so the default of 1 million edits should be on the order of hundreds of MBs or low GBs. NOTE: Fewer extra edits may be retained than value specified for this setting if doing so would mean that more segments would be retained than the number configured by dfs.namenode.max.extra.edits.segments.retained.",
          "default": 1000000
        },
        "max_extra_edits_segments_retained": {
          "type": "integer",
          "description": "The maximum number of extra edit log segments which should be retained beyond what is minimally necessary for a NN restart. When used in conjunction with dfs.namenode.num.extra.edits.retained, this configuration property serves to cap the number of extra edits files to a reasonable value.",
          "default": 10000
        },
        "delegation_key_update-interval": {
          "type": "integer",
          "description": "The update interval for master key for delegation tokens in the namenode in milliseconds.",
          "default": 86400000
        },
        "delegation_token_max-lifetime": {
          "type": "integer",
          "description": "The maximum lifetime in milliseconds for which a delegation token is valid.",
          "default": 604800000
        },
        "delegation_token_renew-interval": {
          "type": "integer",
          "description": "The renewal interval for delegation token in milliseconds.",
          "default": 86400000
        }
      },
      "required": [
        "cpus",
        "mem",
        "disk",
        "disk_type",
        "replication_considerLoad",
        "name_dir_restore",
        "fs-limits_max-component-length",
        "fs-limits_max-directory-items",
        "fs-limits_min-block-size",
        "fs-limits_max-blocks-per-file",
        "acls_enabled",
        "lazypersist_file_scrub_interval_sec",
        "replication_min",
        "handler_count",
        "safemode_threshold-pct",
        "safemode_min_datanodes",
        "safemode_extension",
        "resource_check_interval",
        "resource_du_reserved",
        "resource_checked_volumes",
        "resource_checked_volumes_minimum",
        "max_objects",
        "decommission_interval",
        "decommission_nodes_per_interval",
        "replication_interval",
        "accesstime_precision",
        "plugins",
        "checkpoint_period",
        "checkpoint_txns",
        "checkpoint_check_period",
        "checkpoint_max-retries",
        "num_checkpoints_retained",
        "num_extra_edits_retained",
        "max_extra_edits_segments_retained",
        "delegation_key_update-interval",
        "delegation_token_max-lifetime",
        "delegation_token_renew-interval"
      ]
    },
    "zkfc_node": {
      "description":"HDFS configuration properties.",
      "type": "object",
      "properties": {
        "cpus": {
          "description": "ZKFC node CPU requirement",
          "type": "number",
          "default": 0.3
        },
        "mem": {
          "description": "ZKFC node memory requirement",
          "type": "number",
          "default": 2048
        }
      },
      "required": [
        "cpus",
        "mem"
      ]
    },
    "data_node":{
      "description":"HDFS configuration properties.",
      "type":"object",
      "properties":{
        "count":{
          "description":"Data node count requirement",
          "type":"number",
          "default": 3
        },
        "cpus":{
          "description":"Data node CPU requirement",
          "type":"number",
          "default": 0.3
        },
        "mem":{
          "description":"Data node memory requirement",
          "type":"number",
          "default": 2048
        },
        "disk":{
          "description":"Data node disk size requirement in MB",
          "type":"number",
          "default": 5000
        },
        "disk_type":{
          "description":"Data node disk type",
          "type":"string",
          "default": "ROOT"
        },
        "handler_count": {
          "description": "The number of server threads for the datanode.",
          "type": "integer",
          "default": 10
        },
        "dns_interface": {
          "type": "string",
          "description": "The name of the Network Interface from which a data node should report its IP address.",
          "default": "default"
        },
        "dns_nameserver": {
          "type": "string",
          "description": "The host name or IP address of the name server (DNS) which a DataNode should use to determine the host name used by the NameNode for communication and display purposes.",
          "default": "default"
        },
        "du_reserved": {
          "type": "integer",
          "description": "Reserved space in bytes per volume. Always leave this much space free for non dfs use.",
          "default": 0
        },
        "directoryscan_interval": {
          "type": "integer",
          "description": "Interval in seconds for Datanode to scan data directories and reconcile the difference between blocks in memory and on the disk.",
          "default": 21600
        },
        "directoryscan_threads": {
          "type": "integer",
          "description": "How many threads should the threadpool used to compile reports for volumes in parallel have.",
          "default": 1
        },
        "balance_bandwidthPerSec": {
          "type": "integer",
          "description": "Specifies the maximum amount of bandwidth that each datanode can utilize for the balancing purpose in term of the number of bytes per second.",
          "default": 1048576
        },
        "plugins": {
          "type": "string",
          "description": "Comma-separated list of datanode plug-ins to be activated.",
          "default": ""
        },
        "failed_volumes_tolerated": {
          "type": "integer",
          "description": "The number of volumes that are allowed to fail before a datanode stops offering service. By default any volume failure will cause a datanode to shutdown.",
          "default": 0
        }
      },
      "required": [
        "cpus",
        "mem",
        "disk",
        "disk_type",
        "handler_count",
        "dns_interface",
        "dns_nameserver",
        "du_reserved",
        "directoryscan_interval",
        "directoryscan_threads",
        "balance_bandwidthPerSec",
        "plugins",
        "failed_volumes_tolerated"
      ]
    },
    "hdfs": {
      "type": "object",
      "description": "HDFS File System configuration options",
      "properties": {
        "administrators": {
          "type": "string",
          "description": "Administrators for the HDFS cluster",
          "default": "core,centos,azureuser"
        },
        "name_node_rpc_port": {
          "type": "integer",
          "description": "The RPC port for HDFS Name Nodes.",
          "default": 9001
        },
        "name_node_http_port": {
          "type": "integer",
          "description": "The HTTP port for HDFS Name Nodes. ",
          "default": 9002
        },
        "zkfc_port": {
          "type": "integer",
          "description": "The port for ZKFC Nodes. ",
          "default": 8019
        },
        "journal_node_rpc_port": {
          "type": "integer",
          "description": "The RPC port used by Journal Nodes.",
          "default": 8485
        },
        "journal_node_http_port": {
          "type": "integer",
          "description": "The HTTP port used by Journal Nodes.",
          "default": 8480
        },
        "data_node_rpc_port": {
          "type": "integer",
          "description": "The RPC port used by Data Nodes.",
          "default": 9003
        },
        "data_node_http_port": {
          "type": "integer",
          "description": "The HTTP port used by Data Nodes.",
          "default": 9004
        },
        "data_node_ipc_port": {
          "type": "integer",
          "description": "The IPS port used by Data Nodes.",
          "default": 9005
        },
        "permissions_enabled": {
          "type": "boolean",
          "description": "If true, permissions checking is enabled",
          "default": false
        },
        "name_node_heartbeat_recheck_interval": {
          "type": "integer",
          "description": "This time decides the interval to check for expired datanodes.",
          "default": 60000
        },
        "compress_image": {
          "type": "boolean",
          "description": "If true, the File System image will be compressed.",
          "default": true
        },
        "image_compression_codec": {
          "type": "string",
          "description": "The image compression codec for the File System image.",
          "default": "org.apache.hadoop.io.compress.SnappyCodec"
        },
        "hadoop_root_logger": {
          "type": "string",
          "description": "",
          "default": "INFO,console"
        },
        "ipc_client_connect_max_retries": {
          "type": "integer",
          "description": "Indicates the number of retries a client will make to establish a server connection.",
          "default": 300
        },
        "namenode_logging_level": {
          "type": "string",
          "description": "The logging level for dfs namenode. Other values are 'dir' (trace namespace mutations), 'block'' (trace block under/over replications and block creations/deletions), or 'all'.",
          "default": "info"
        },
        "namenode_rpc-bind-host_name_node_0": {
          "type": "string",
          "description": "The actual address the RPC server will bind to. If this optional address is set, it overrides only the hostname portion of dfs.namenode.rpc-address. This is useful for making the name node listen on all interfaces by setting it to 0.0.0.0.",
          "default": "0.0.0.0"
        },
        "namenode_rpc-bind-host_name_node_1": {
          "type": "string",
          "description": "The actual address the RPC server will bind to. If this optional address is set, it overrides only the hostname portion of dfs.namenode.rpc-address. This is useful for making the name node listen on all interfaces by setting it to 0.0.0.0.",
          "default": "0.0.0.0"
        },
        "namenode_http-bind-host_name_node_0": {
          "type": "string",
          "description": "The actual adress the HTTP server will bind to. If this optional address is set, it overrides only the hostname portion of dfs.namenode.http-address. This is useful for making the name node HTTP server listen on all interfaces by setting it to 0.0.0.0.",
          "default": "0.0.0.0"
        },
        "namenode_http-bind-host_name_node_1": {
          "type": "string",
          "description": "The actual adress the HTTP server will bind to. If this optional address is set, it overrides only the hostname portion of dfs.namenode.http-address. This is useful for making the name node HTTP server listen on all interfaces by setting it to 0.0.0.0.",
          "default": "0.0.0.0"
        },
        "hadoop_hdfs_configuration_version": {
          "type": "integer",
          "description": "version of this configuration file",
          "default": 1
        },
        "namenode_servicerpc_address_namenode_0": {
          "type": "string",
          "description": "RPC address for HDFS Services communication. BackupNode, Datanodes and all other services should be connecting to this address if it is configured. If the value of this property is unset the value of dfs.namenode.rpc-address will be used as the default.",
          "default": ""
        },
        "namenode_servicerpc_address_namenode_1": {
          "type": "string",
          "description": "RPC address for HDFS Services communication. BackupNode, Datanodes and all other services should be connecting to this address if it is configured. If the value of this property is unset the value of dfs.namenode.rpc-address will be used as the default.",
          "default": ""
        },
        "namenode_servicerpc_bind_host_namenode_0": {
          "type": "string",
          "description": "The actual address the service RPC server will bind to. If this optional address is set, it overrides only the hostname portion of dfs.namenode.servicerpc-address. This is useful for making the name node listen on all interfaces by setting it to 0.0.0.0.",
          "default": ""
        },
        "namenode_servicerpc_bind_host_namenode_1": {
          "type": "string",
          "description": "The actual address the service RPC server will bind to. If this optional address is set, it overrides only the hostname portion of dfs.namenode.servicerpc-address. This is useful for making the name node listen on all interfaces by setting it to 0.0.0.0.",
          "default": ""
        },
        "client_cached_conn_retry": {
          "type": "integer",
          "description": "The number of times the HDFS client will pull a socket from the cache. Once this number is exceeded, the client will try to create a new socket.",
          "default": 3
        },
        "https_server_keystore_resource": {
          "type": "string",
          "description": "Resource file from which ssl server keystore information will be extracted",
          "default": "ssl-server.xml"
        },
        "client_https_keystore_resource": {
          "type": "string",
          "description": "Resource file from which ssl client keystore information will be extracted",
          "default": "ssl-client.xml"
        },
        "default_chunk_view_size": {
          "type": "integer",
          "description": "The number of bytes to view for a file on the browser.",
          "default": 32768
        },
        "permissions_superusergroup": {
          "type": "string",
          "description": "The name of the group of super-users.",
          "default": "supergroup"
        },
        "block_access_token_enable": {
          "type": "boolean",
          "description": "If 'true', access tokens are used as capabilities for accessing datanodes. If 'false', no access tokens are checked on accessing datanodes.",
          "default": false
        },
        "block_access_key_update_interval": {
          "type": "integer",
          "description": "Interval in minutes at which namenode updates its access keys.",
          "default": 600
        },
        "block_access_token_lifetime": {
          "type": "integer",
          "description": "The lifetime of access tokens in minutes.",
          "default": 600
        },
        "replication": {
          "type": "integer",
          "description": "Default block replication. The actual number of replications can be specified when the file is created. The default is used if replication is not specified in create time.",
          "default": 3
        },
        "replication_max": {
          "type": "integer",
          "description": "Maximal block replication.",
          "default": 512
        },
        "blocksize": {
          "type": "integer",
          "description": "The default block size for new files, in bytes. You can use the following suffix (case insensitive): k(kilo), m(mega), g(giga), t(tera), p(peta), e(exa) to specify the size (such as 128k, 512m, 1g, etc.), Or provide complete size in bytes (such as 134217728 for 128 MB).",
          "default": 134217728
        },
        "client_block_write_retries": {
          "type": "integer",
          "description": "The number of retries for writing blocks to the data nodes, before we signal failure to the application.",
          "default": 3
        },
        "client_block_write_replace-datanode-on-failure_enable": {
          "type": "boolean",
          "description": "If there is a datanode/network failure in the write pipeline, DFSClient will try to remove the failed datanode from the pipeline and then continue writing with the remaining datanodes. As a result, the number of datanodes in the pipeline is decreased. The feature is to add new datanodes to the pipeline. This is a site-wide property to enable/disable the feature. When the cluster size is extremely small, e.g. 3 nodes or less, cluster administrators may want to set the policy to NEVER in the default configuration file or disable this feature. Otherwise, users may experience an unusually high rate of pipeline failures since it is impossible to find new datanodes for replacement. See also dfs.client.block.write.replace-datanode-on-failure.policy",
          "default": true
        },
        "client_block_write_replace-datanode-on-failure_policy": {
          "type": "string",
          "description": "This property is used only if the value of dfs.client.block.write.replace-datanode-on-failure.enable is true. ALWAYS: always add a new datanode when an existing datanode is removed. NEVER: never add a new datanode. DEFAULT: Let r be the replication number. Let n be the number of existing datanodes. Add a new datanode only if r is greater than or equal to 3 and either (1) floor(r/2) is greater than or equal to n; or (2) r is greater than n and the block is hflushed/appended.",
          "default": "DEFAULT"
        },
        "client_block_write_replace-datanode-on-failure_best-effort": {
          "type": "boolean",
          "description": "This property is used only if the value of dfs.client.block.write.replace-datanode-on-failure.enable is true. Best effort means that the client will try to replace a failed datanode in write pipeline (provided that the policy is satisfied), however, it continues the write operation in case that the datanode replacement also fails. Suppose the datanode replacement fails. false: An exception should be thrown so that the write will fail. true : The write should be resumed with the remaining datandoes. Note that setting this property to true allows writing to a pipeline with a smaller number of datanodes. As a result, it increases the probability of data loss.",
          "default": false
        },
        "blockreport_intervalMsec": {
          "type": "integer",
          "description": "Determines block reporting interval in milliseconds.",
          "default": 21600000
        },
        "blockreport_initialDelay": {
          "type": "integer",
          "description": "Delay for first block report in seconds.",
          "default": 0
        },
        "blockreport_split_threshold": {
          "type": "integer",
          "description": "If the number of blocks on the DataNode is below this threshold then it will send block reports for all Storage Directories in a single message. If the number of blocks exceeds this threshold then the DataNode will send block reports for each Storage Directory in separate messages. Set to zero to always split.",
          "default": 1000000
        },
        "heartbeat_interval": {
          "type": "integer",
          "description": "Determines datanode heartbeat interval in seconds.",
          "default": 3
        },
        "hosts": {
          "type": "string",
          "description": "Names a file that contains a list of hosts that are permitted to connect to the namenode. The full pathname of the file must be specified. If the value is empty, all hosts are permitted.",
          "default": ""
        },
        "hosts_exclude": {
          "type": "string",
          "description": "Names a file that contains a list of hosts that are not permitted to connect to the namenode. The full pathname of the file must be specified. If the value is empty, no hosts are excluded.",
          "default": ""
        },
        "namenode_datanode_registration_ip-hostname-check": {
          "type": "boolean",
          "description": "If true (the default), then the namenode requires that a connecting datanode's address must be resolved to a hostname. If necessary, a reverse DNS lookup is performed. All attempts to register a datanode from an unresolvable address are rejected. It is recommended that this setting be left on to prevent accidental registration of datanodes listed by hostname in the excludes file during a DNS outage. Only set this to false in environments where there is no infrastructure to support reverse DNS lookup.",
          "default": true
        },
        "stream-buffer-size": {
          "type": "integer",
          "description": "The size of buffer to stream files. The size of this buffer should probably be a multiple of hardware page size (4096 on Intel x86), and it determines how much data is buffered during read and write operations.",
          "default": 4096
        },
        "bytes-per-checksum": {
          "type": "integer",
          "description": "The number of bytes per checksum. Must not be larger than dfs.stream-buffer-size",
          "default": 512
        },
        "client-write-packet-size": {
          "type": "integer",
          "description": "Packet size for clients to write.",
          "default": 65536
        },
        "client_write_exclude_nodes_cache_expiry_interval_millis": {
          "type": "integer",
          "description": "The maximum period to keep a DN in the excluded nodes list at a client. After this period, in milliseconds, the previously excluded node(s) will be removed automatically from the cache and will be considered good for block allocations again. Useful to lower or raise in situations where you keep a file open for very long periods (such as a Write-Ahead-Log (WAL) file) to make the writer tolerant to cluster maintenance restarts. Defaults to 10 minutes.",
          "default": 600000
        },
        "image_compress": {
          "type": "boolean",
          "description": "Should the dfs image be compressed?",
          "default": false
        }
      },
      "required": [
        "name_node_rpc_port",
        "name_node_http_port",
        "journal_node_rpc_port",
        "journal_node_http_port",
        "data_node_rpc_port",
        "data_node_http_port",
        "data_node_ipc_port",
        "permissions_enabled",
        "name_node_heartbeat_recheck_interval",
        "compress_image",
        "image_compression_codec",
        "hadoop_root_logger",
        "ipc_client_connect_max_retries",
        "namenode_logging_level",
        "namenode_rpc-bind-host_name_node_0",
        "namenode_rpc-bind-host_name_node_1",
        "namenode_http-bind-host_name_node_0",
        "namenode_http-bind-host_name_node_1",
        "hadoop_hdfs_configuration_version",
        "namenode_servicerpc_address_namenode_0",
        "namenode_servicerpc_address_namenode_1",
        "namenode_servicerpc_bind_host_namenode_0",
        "namenode_servicerpc_bind_host_namenode_1",
        "client_cached_conn_retry",
        "default_chunk_view_size",
        "permissions_superusergroup",
        "block_access_token_enable",
        "block_access_key_update_interval",
        "block_access_token_lifetime",
        "replication",
        "replication_max",
        "blocksize",
        "client_block_write_retries",
        "client_block_write_replace-datanode-on-failure_enable",
        "client_block_write_replace-datanode-on-failure_policy",
        "client_block_write_replace-datanode-on-failure_best-effort",
        "blockreport_intervalMsec",
        "blockreport_initialDelay",
        "blockreport_split_threshold",
        "heartbeat_interval",
        "hosts",
        "hosts_exclude",
        "namenode_datanode_registration_ip-hostname-check",
        "stream-buffer-size",
        "bytes-per-checksum",
        "client-write-packet-size",
        "client_write_exclude_nodes_cache_expiry_interval_millis",
        "image_compress"
      ]
    }
  }
}
